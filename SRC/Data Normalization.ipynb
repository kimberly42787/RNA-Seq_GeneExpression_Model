{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "#### In order to compare our data from each other, we need to normalize our data to accurately compare them. \n",
    "\n",
    "There are two ways we can normalize our data: \n",
    "- **Min-Max Scaling:**\n",
    "    - Transform data to a specific range, usually [0,1]. \n",
    "    - Sensitive to outliers\n",
    "    - This normalization is suitable for algorithms that assume features are on sim,ilar scale, such as neural networks and algorithms using distance-based metrics.\n",
    "- **Z-Score Normalization:**\n",
    "    - Centers the data around 0 with a standard deviation of 1. \n",
    "    - Less sensitive to outliers compared to Min-Max Scaling.\n",
    "    - Suitable for algorithms that assume normally distributed data or require features to have zero mean and unit variance, such as linear regression, support vector machines, and k-means clustering. \n",
    "\n",
    "I am going to run both normalization algorithm to explore it a little further. \n",
    "- Packages needed: \n",
    "    - from sklearn.preprocessing import MinMaxScaler\n",
    "    - from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "# Initial Packages\n",
    "import pandas as pd\n",
    "from functions import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Categorical to Numeric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# SMOTE for imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling \n",
    "\n",
    "# First, separate our numeric columns and non-numeric_columns as we will only need to apply our normalization to all the numeric columns\n",
    "mm_non_numeric_cols = updated_df.select_dtypes(exclude = ['float64']).columns\n",
    "mm_numeric_cols = updated_df.select_dtypes(include=['float64']).columns\n",
    "mm_numeric_data = updated_df[mm_numeric_cols]\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Data Transformation\n",
    "min_max_array = scaler.fit_transform(mm_numeric_data) # Will spit out a numpy array\n",
    "\n",
    "# Convert to a dataframe\n",
    "min_max_df = pd.DataFrame(min_max_array, columns = mm_numeric_cols)\n",
    "\n",
    "norm_min_max_df = pd.concat([updated_df[mm_non_numeric_cols], min_max_df], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation and create subset dataset for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab only the numeric columns\n",
    "correlation_columns = norm_min_max_df.iloc[:, 2:].select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Check the correlation between each numeric colunm with the encoded target variable\n",
    "correlations = norm_min_max_df[correlation_columns].corrwith(norm_min_max_df['class_encoded'])\n",
    "\n",
    "# Order the correlation by ordering the absolute values\n",
    "important_m_corr= correlations.abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Series into a Dataframe\n",
    "mm_corr_df = pd.DataFrame(important_m_corr)\n",
    "mm_corr_df.reset_index(inplace=True)\n",
    "mm_corr_df.columns = ['Gene', 'Correlation']\n",
    "\n",
    "# Save dataframe to a csv file\n",
    "subfolder_path = \"/Users/kim/Desktop/repos/RNA-Seq_GeneExpression_Model/Datasets\"\n",
    "csv_filename = \"m_correlation_results.csv\"\n",
    "save_dataframe_to_csv(mm_corr_df, subfolder_path, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that I have my correlation data. I will create subsets of my original normalized dataframe that only contains genes with a certain correlation.\n",
    "\n",
    "Here are my statistics for my correlation data: \n",
    "- Min: 0.000020\n",
    "- Max: 0.8500829894215582\n",
    "\n",
    "Percentiles: \n",
    "- 25%\t0.064557\n",
    "- 50%\t0.147424\n",
    "- 75%\t0.262105\n",
    "\n",
    "The three correlation threshold I will work with are: \n",
    "- 0.262105\n",
    "- 0.50\n",
    "- 0.75\n",
    "\n",
    "**All three subsets will be above the 75% percentile.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Thresholds\n",
    "corr_threshold_1 = 0.262105 #75 percentile\n",
    "corr_threshold_2 = 0.50\n",
    "corr_threshold_3 = 0.75\n",
    "\n",
    "# Return subset dataframes based on correlation threshold\n",
    "min_max_subset_df_1 = df_corr_subset(norm_min_max_df, mm_corr_df, corr_threshold_1)\n",
    "min_max_subset_df_2 = df_corr_subset(norm_min_max_df, mm_corr_df, corr_threshold_2)\n",
    "min_max_subset_df_3 = df_corr_subset(norm_min_max_df, mm_corr_df, corr_threshold_3)\n",
    "\n",
    "# Save dataframes to a csv file in our dataset subfolder\n",
    "subfolder_path = \"/Users/kim/Desktop/repos/RNA-Seq_GeneExpression_Model/Datasets\"\n",
    "csv_filename_1 = \"min-max_threshold_df_0.26.csv\"\n",
    "csv_filename_2 = \"min-max_threshold_df_0.50.csv\"\n",
    "csv_filename_3 = \"min-max_threshold_df_0.75.csv\"\n",
    "\n",
    "save_dataframe_to_csv(min_max_subset_df_1, subfolder_path, csv_filename_1)\n",
    "save_dataframe_to_csv(min_max_subset_df_2, subfolder_path, csv_filename_2)\n",
    "save_dataframe_to_csv(min_max_subset_df_3, subfolder_path, csv_filename_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subset_df_1 contains 5069 columns \n",
    "- Subset_df_2 contains 698 columns \n",
    "- Subset_df_3 contains 13 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Normalization (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score Normalization\n",
    "\n",
    "# First, separate our numeric columns and non-numeric_columns as we will only need to apply our normalization to all the numeric columns\n",
    "\n",
    "z_non_numeric_cols = updated_df.select_dtypes(exclude = ['float64']).columns\n",
    "z_numeric_cols = updated_df.select_dtypes(include=['float64']).columns\n",
    "z_numeric_data = updated_df[z_numeric_cols]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "z_normalized_array = scaler.fit_transform(z_numeric_data)\n",
    "\n",
    "# Convert to a dataframe\n",
    "z_df = pd.DataFrame(z_normalized_array, columns = z_numeric_cols)\n",
    "\n",
    "norm_z_df = pd.concat([updated_df[z_non_numeric_cols], z_df], axis = 1)\n",
    "norm_z_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "LE = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the data\n",
    "norm_z_df['class_encoded'] = LE.fit_transform(norm_z_df['Class'])\n",
    "\n",
    "# Print to check \n",
    "print(norm_z_df['class_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab only the numeric columns\n",
    "correlation_columns = norm_z_df.iloc[:, 2:].select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Check the correlation between each numeric colunm with the encoded target variable\n",
    "correlations = norm_z_df[correlation_columns].corrwith(norm_z_df['class_encoded'])\n",
    "\n",
    "# Order the correlation by ordering the absolute values\n",
    "important_z_corr= correlations.abs().sort_values(ascending = False)\n",
    "\n",
    "# Convert Series into a Dataframe\n",
    "z_corr_df = pd.DataFrame(important_z_corr)\n",
    "z_corr_df.reset_index(inplace=True)\n",
    "z_corr_df.columns = ['Gene', 'Correlation']\n",
    "\n",
    "# Save dataframe to a csv file\n",
    "subfolder_path = \"/Users/kim/Desktop/repos/RNA-Seq_GeneExpression_Model/Datasets\"\n",
    "csv_filename = \"z_correlation_results.csv\"\n",
    "save_dataframe_to_csv(z_corr_df, subfolder_path, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two correlation dataframe using the two normalization algorithm \n",
    " \n",
    "# Merge dataframes \n",
    "merged_corr_df = pd.merge(mm_corr_df, z_corr_df, on = 'Gene', suffixes= ('_mm_corr_df', '_z_corr_df'), how = 'outer', indicator = True)\n",
    "\n",
    "# Filter rows where values are different\n",
    "differing_rows = merged_corr_df[merged_corr_df['_merge'] != 'both']\n",
    "\n",
    "# Print the differing rows\n",
    "print(differing_rows)\n",
    "merged_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalization Summary: \n",
    "\n",
    "I employed both the Min-Max Scaling and Z-Score Normalization algorithms to standardize my dataset, subsequently correlating the normalized data with my target variable.\n",
    "\n",
    "The analysis revealed distinct normalization outputs from the two algorithms, as expected due to their differing calculation methodologies.\n",
    "\n",
    "To assess consistency, I compared the genes exhibiting the highest correlation with the target variable under Min-Max Scaling with those identified using Z-Score Normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
