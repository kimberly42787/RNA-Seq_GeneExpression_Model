{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene Expression RNA-Seq Classification \n",
    " \n",
    "#### Classification Models\n",
    "I will be working with the subsets of my original normalized dataframe that only contains genes with a certain correlation.\n",
    "\n",
    "The three correlation threshold I will work with are: \n",
    "- 0.262105\n",
    "- 0.50\n",
    "- 0.75\n",
    "\n",
    "These will be the datasets I will work with first to explore different models and then I will re-adjust my datasets if needed to terst out different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of classification models:\n",
    "- Binary\n",
    "- Multi-Class\n",
    "- Multi-Label\n",
    "- Imbalanced Classifications\n",
    "\n",
    "I am currently exploring various classification models for my dataset, which primarily aligns with Multi-Class Classification. I aim to determine the most suitable algorithm among the available Multi-Class options.\n",
    "\n",
    "In addition to Multi-Class Classification, I am intrigued by the idea of testing the dataset with a Multi-Label algorithm. Given that my dataset involves gene expression levels and tumor classification, I am particularly interested in understanding how the model assigns percentages to specific tumor types compared to others. This approach will provide insights into the nuanced relationships between gene expressions and different tumor classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "\n",
    "#Import functions\n",
    "from functions import *\n",
    "\n",
    "# Train and Test package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "# import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreiving our dataset and splitting into our train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our training datasets \n",
    "subset_data_1 = pd.read_csv(\"/Users/kim/Desktop/repos/RNA-Seq_GeneExpression_Model/Datasets/min-max_threshold_df_0.26.csv\")\n",
    "subset_data_2 = pd.read_csv(\"/Users/kim/Desktop/repos/RNA-Seq_GeneExpression_Model/Datasets/min-max_threshold_df_0.50.csv\")\n",
    "subset_data_3 = pd.read_csv(\"/Users/kim/Desktop/repos/RNA-Seq_GeneExpression_Model/Datasets/min-max_threshold_df_0.75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our 3 dataframes to features(X) and target(Y)\n",
    "x_train_1, x_test_1, x_val_1, y_train_1, y_test_1, y_val_1 = split_train_test_data(subset_data_1)\n",
    "x_train_2, x_test_2, x_val_2, y_train_2, y_test_2, y_val_2 = split_train_test_data(subset_data_2)\n",
    "x_train_3, x_test_3, x_val_3, y_train_3, y_test_3, y_val_3 = split_train_test_data(subset_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Find the ideal parameters to run the Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV for hyperparameter tuning \n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "best_params1 = hyperparameter_grid_search(x_val_1, y_val_1, param_grid)\n",
    "# best_params2 = hyperparameter_grid_search(x_train_2, y_train_2, param_grid)\n",
    "# best_params3 = hyperparameter_grid_search(x_train_3, y_train_3, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 0, 4, 0, 2, 0, 0, 0, 4, 0, 2, 4, 0, 2, 0, 2, 4, 4, 2, 2,\n",
       "       0, 4, 3, 2, 4, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 3, 1, 2, 0,\n",
       "       4, 2, 4, 4, 3, 0, 3, 0, 0, 2, 0, 0, 0, 4, 2, 4, 3, 0, 0, 2, 0, 2,\n",
       "       0, 3, 0, 1, 0, 0, 0, 0, 0, 4, 4, 4, 0, 3, 0, 3, 0, 1, 4, 1, 0, 0,\n",
       "       2, 0, 0, 0, 1, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializae RandomForest Classifier with the best parameter\n",
    "rf_model1 = RandomForestClassifier(**best_params1, random_state = 42) # **best_params unpacks the dictionary with the hyperparameter values\n",
    "\n",
    "# Train the model \n",
    "rf_model1.fit(x_train_1, y_train_1)\n",
    "\n",
    "# Test model by making prediction using the validation data\n",
    "y_pred1 = rf_model1.predict(x_test_1)\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model1: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Check Accuracy\n",
    "accuracy1 = accuracy_score(y_test_1, y_pred1)\n",
    "print(f\"Accuracy of Model1: {accuracy1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[45  0  0  0  0]\n",
      " [ 0  5  0  0  0]\n",
      " [ 0  0 19  0  0]\n",
      " [ 0  0  0  9  0]\n",
      " [ 0  0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are your true labels and predicted labels\n",
    "conf_matrix = confusion_matrix(y_test_1, y_pred1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        45\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00        19\n",
      "           3       1.00      1.00      1.00         9\n",
      "           4       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        98\n",
      "   macro avg       1.00      1.00      1.00        98\n",
      "weighted avg       1.00      1.00      1.00        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC-ROC: 1.0\n",
      "Class-wise AUC-ROC:\n",
      "Class 0: 1.0\n",
      "Class 1: 1.0\n",
      "Class 2: 1.0\n",
      "Class 3: 1.0\n",
      "Class 4: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "y_pred_proba1 = rf_model1.predict_proba(x_test_1)\n",
    "\n",
    "# Calculate AUC-ROC for each class\n",
    "class_auc_roc = {}\n",
    "for i in range(y_pred_proba1.shape[1]):\n",
    "    y_true_class = (y_test_1 == i).astype(int)\n",
    "    y_probs_class = y_pred_proba1[:, i]\n",
    "    auc_roc_class = roc_auc_score(y_true_class, y_probs_class)\n",
    "    class_auc_roc[i] = auc_roc_class\n",
    "\n",
    "# Overall AUC-ROC for multiclass classification\n",
    "overall_auc_roc = roc_auc_score(y_test_1, y_pred_proba1, multi_class='ovr')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Overall AUC-ROC: {overall_auc_roc}\")\n",
    "print(\"Class-wise AUC-ROC:\")\n",
    "for class_label, auc_roc_class in class_auc_roc.items():\n",
    "    print(f\"Class {class_label}: {auc_roc_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Precision, Recall, F1 Score\n",
    "    precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "    recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    # ROC AUC Score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
